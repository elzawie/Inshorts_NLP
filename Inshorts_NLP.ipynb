{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# General information\n",
    "\n",
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "In this Jupyter Notebook we will be working with news articles retrieved from <a href='https://inshorts.com'> Inshorts.com</a> a website that gives us short, 60-word news articles on a wide variety of topics. Inshorts provides news for 12 thematically different sets of content like: Sports, Politics, Business and Technology.\n",
    "\n",
    "The main focus is to cover some aspects of NLP like:\n",
    "- Data Retrieval with Web Scraping\n",
    "- Text wrangling and pre-processing\n",
    "- Parts of Speech tagging + visualizing dependencies\n",
    "- Named Entity Recognition\n",
    "- Building a classifier able to recognize type of content based on words used in the article\n",
    "\n",
    "Let's start off by importing all necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Basic modules for dataframe manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Web scraping\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Retrieving data\n",
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 17,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "In this section we will focus on creating a function for gathering news data from <a href='https://inshorts.com'>Inshorts.com</a>. Since articles are divided by topic and each category displays only ~ 25 articles, we need a solution which will trigger the 'Load more' button desired number of times before retrieving the data and creating a DataFrame, we will achieve this by utilizing Selenium - a popular testing framework - and ChromeDriver. For those who prefer Firefox over Chrome, there is a possibility to use Mozilla's GeckoDriver with Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "source_urls = ['https://inshorts.com/en/read/business',\n",
    "               'https://inshorts.com/en/read/technology',\n",
    "               'https://inshorts.com/en/read/science',\n",
    "               'https://inshorts.com/en/read/world',\n",
    "               'https://inshorts.com/en/read/sports',\n",
    "               'https://inshorts.com/en/read/politics',\n",
    "               'https://inshorts.com/en/read/entertainment',\n",
    "               'https://inshorts.com/en/read/hatke',\n",
    "               'https://inshorts.com/en/read/automobile']\n",
    "\n",
    "PATH = 'chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_data(seed_urls):\n",
    "    news_data = []\n",
    "    for url in seed_urls:\n",
    "        # Creating a new Chrome session\n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(url)\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                python_button = driver.find_element_by_id('load-more-btn') # Find 'Load more' button\n",
    "                sleep(2)\n",
    "                python_button.click() # Click 'Load more' button to load more articles\n",
    "                sleep(5)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "        news_category = url.split('/')[-1]\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')     \n",
    "        news_articles = [{'news_headline': headline.find('span', \n",
    "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
    "                          'news_article': article.find('div', \n",
    "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                          'news_category': news_category}\n",
    "                         \n",
    "                            for headline, article in \n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        driver.quit()\n",
    "    df =  pd.DataFrame(news_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df_news = get_data(source_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 14,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When deciding the right term insurance plan, c...</td>\n",
       "      <td>business</td>\n",
       "      <td>Max Life Insurance delivering high claims paid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra Pradesh has topped the ease of doing bu...</td>\n",
       "      <td>business</td>\n",
       "      <td>Andhra Pradesh tops ease of doing business ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rajinikanth's wife Latha Rajinikanth will have...</td>\n",
       "      <td>business</td>\n",
       "      <td>Rajinikanth's wife to face trial for fraud ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online payments company PayPal's UK unit sent ...</td>\n",
       "      <td>business</td>\n",
       "      <td>Being dead is breach of contract, PayPal tells...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The world's fifth largest smartphone seller, C...</td>\n",
       "      <td>business</td>\n",
       "      <td>World's 5th largest smartphone seller Xiaomi m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        news_article news_category  \\\n",
       "0  When deciding the right term insurance plan, c...      business   \n",
       "1  Andhra Pradesh has topped the ease of doing bu...      business   \n",
       "2  Rajinikanth's wife Latha Rajinikanth will have...      business   \n",
       "3  Online payments company PayPal's UK unit sent ...      business   \n",
       "4  The world's fifth largest smartphone seller, C...      business   \n",
       "\n",
       "                                       news_headline  \n",
       "0  Max Life Insurance delivering high claims paid...  \n",
       "1  Andhra Pradesh tops ease of doing business ran...  \n",
       "2  Rajinikanth's wife to face trial for fraud ove...  \n",
       "3  Being dead is breach of contract, PayPal tells...  \n",
       "4  World's 5th largest smartphone seller Xiaomi m...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing first 5 rows of data\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 7,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         564\n",
       "politics         563\n",
       "world            563\n",
       "entertainment    560\n",
       "science          560\n",
       "sports           558\n",
       "hatke            557\n",
       "technology       556\n",
       "automobile       553\n",
       "Name: news_category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the distribution of each topic in our DataFrame\n",
    "df_news['news_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Saving data into a '.csv' file to prevent redundant overloading of the Inshort's server.\n",
    "df_news.to_csv('news.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Text pre-processing\n",
    "\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Loading one of English language models for spacy\n",
    "nlp = spacy.load('en_core_web_md', parse = True, tag = True, entity = True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Saving english stopwords from nltk module in a list\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Removing 'no' and 'not' from stopwords list\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Writing helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 25,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 35,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Let's start off our data preparation by writing some helper functions which we will use to clean the news data. Since HTML tags don't add much value towards understanding and analyzing text, we will get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Removing accented characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 39,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "While dealing with text data, very often we encounter accented characters like 'é' or 'ó'. Since they may not be useful while working with English language, we will create function for converting them into unaccented counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 29,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Expanding contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 43,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "As a next step, let's create a function for dealing with contractions. Contractions are shortened versions of words or syllabes which often exist in written or spoken forms in English language. The typical examples would be <b>do not</b> to <b>don't</b> and <b>I would</b> to <b>I'd</b>. For this purpose we will utilize function and contractions dictionary written by Dipanjan S - Data Scientist working for Intel company, author of <i>Text analytics with Python</i> and <i>Practical machine learning with Python</i>.\n",
    "\n",
    "<a href=\"https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/nlp%20proven%20approach\">Contraction dictionary by Dipanjan S</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 47,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 51,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Special characters are usually non-alphanumeric or even numeric characters which constitute to the extra noise in unstructured text data. We will create a function based on simple regular expressions which will get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 47,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 55,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. It depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. For this part we will utilize <b>Spacy</b> as it has excellent built-in lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 47,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 59,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "In computing, <i>stopwords</i> are words which are filtered out before or after processing of natural language data. Though stopwords usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. Some examples include <b>a</b>, <b>an</b>, <b>the</b>, <b>and</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 63,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Combining above functions - building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # Normalizing each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # Stripping HTML\n",
    "        if html_stripping:\n",
    "            doc = remove_html_tags(doc)\n",
    "        # Removing accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # Expanding contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # Lowering the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # Removing extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # Lemmatizing text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # Removing special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # Inserting spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # Removing extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # Removing stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 17,
        "hidden": false,
        "row": 63,
        "width": 5
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When deciding the right term insurance plan, c...</td>\n",
       "      <td>business</td>\n",
       "      <td>Max Life Insurance delivering high claims paid...</td>\n",
       "      <td>Max Life Insurance delivering high claims paid...</td>\n",
       "      <td>max life insurance deliver high claim pay year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra Pradesh has topped the ease of doing bu...</td>\n",
       "      <td>business</td>\n",
       "      <td>Andhra Pradesh tops ease of doing business ran...</td>\n",
       "      <td>Andhra Pradesh tops ease of doing business ran...</td>\n",
       "      <td>andhra pradesh top ease business rank nd year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rajinikanth's wife Latha Rajinikanth will have...</td>\n",
       "      <td>business</td>\n",
       "      <td>Rajinikanth's wife to face trial for fraud ove...</td>\n",
       "      <td>Rajinikanth's wife to face trial for fraud ove...</td>\n",
       "      <td>rajinikanth wife face trial fraud cr unpaid du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online payments company PayPal's UK unit sent ...</td>\n",
       "      <td>business</td>\n",
       "      <td>Being dead is breach of contract, PayPal tells...</td>\n",
       "      <td>Being dead is breach of contract, PayPal tells...</td>\n",
       "      <td>dead breach contract paypal tell deceased pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The world's fifth largest smartphone seller, C...</td>\n",
       "      <td>business</td>\n",
       "      <td>World's 5th largest smartphone seller Xiaomi m...</td>\n",
       "      <td>World's 5th largest smartphone seller Xiaomi m...</td>\n",
       "      <td>world th large smartphone seller xiaomi make i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        news_article news_category  \\\n",
       "0  When deciding the right term insurance plan, c...      business   \n",
       "1  Andhra Pradesh has topped the ease of doing bu...      business   \n",
       "2  Rajinikanth's wife Latha Rajinikanth will have...      business   \n",
       "3  Online payments company PayPal's UK unit sent ...      business   \n",
       "4  The world's fifth largest smartphone seller, C...      business   \n",
       "\n",
       "                                       news_headline  \\\n",
       "0  Max Life Insurance delivering high claims paid...   \n",
       "1  Andhra Pradesh tops ease of doing business ran...   \n",
       "2  Rajinikanth's wife to face trial for fraud ove...   \n",
       "3  Being dead is breach of contract, PayPal tells...   \n",
       "4  World's 5th largest smartphone seller Xiaomi m...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Max Life Insurance delivering high claims paid...   \n",
       "1  Andhra Pradesh tops ease of doing business ran...   \n",
       "2  Rajinikanth's wife to face trial for fraud ove...   \n",
       "3  Being dead is breach of contract, PayPal tells...   \n",
       "4  World's 5th largest smartphone seller Xiaomi m...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  max life insurance deliver high claim pay year...  \n",
       "1  andhra pradesh top ease business rank nd year ...  \n",
       "2  rajinikanth wife face trial fraud cr unpaid du...  \n",
       "3  dead breach contract paypal tell deceased pati...  \n",
       "4  world th large smartphone seller xiaomi make i...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining headline and article text\n",
    "df_news['full_text'] = df_news[\"news_headline\"].map(str)+ '. ' + df_news[\"news_article\"]\n",
    "\n",
    "# Pre-processing text and store the same\n",
    "df_news['clean_text'] = normalize_corpus(df_news['full_text'])\n",
    "norm_corpus = list(df_news['clean_text'])\n",
    "\n",
    "# Displaying first 5 rows\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Saving preprocessed DataFrame into .csv file\n",
    "df_news.to_csv('news_preprocessed.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 67,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Parts of Speech tagging\n",
    "\n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 16,
        "hidden": false,
        "row": 80,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Parts of speech (POS) are specific lexical categories to which words are assigned, based on their syntactic context and role. Usually, words can fall into one of the following major categories.\n",
    "\n",
    "- <b>Noun</b>: This usually denotes words that depict some object or entity, which may be living or nonliving. Some examples would be fox , dog , book , and so on. The POS tag symbol for nouns is N.\n",
    "\n",
    "- <b>Verb</b>: Verbs are words that are used to describe certain actions, states, or occurrences. There are a wide variety of further subcategories, such as auxiliary, reflexive, and transitive verbs (and many more). Some typical examples of verbs would be running , jumping , read , and write . The POS tag symbol for verbs is V.\n",
    "\n",
    "- <b>Adjective</b>: Adjectives are words used to describe or qualify other words, typically nouns and noun phrases. The phrase beautiful flower has the noun (N) flower which is described or qualified using the adjective (ADJ) beautiful . The POS tag symbol for adjectives is ADJ .\n",
    "\n",
    "- <b>Adverb</b>: Adverbs usually act as modifiers for other words including nouns, adjectives, verbs, or other adverbs. The phrase very beautiful flower has the adverb (ADV) very , which modifies the adjective (ADJ) beautiful , indicating the degree to which the flower is beautiful. The POS tag symbol for adverbs is ADV.\n",
    "\n",
    "Besides these four major categories of parts of speech , there are other categories that occur frequently in the English language. These include pronouns, prepositions, interjections, conjunctions, determiners, and many others. Furthermore, each POS tag like the noun (N) can be further subdivided into categories like singular nouns (NN), singular proper nouns (NNP), and plural nouns (NNS).\n",
    "\n",
    "\n",
    "\n",
    "Since in this Jupyter Notebook we are going to show examples of Parts of Speech tagging and Named Entity Recognition using Spacy with 'en_core_web_md' model, we will apply them on the unprocessed data as it was proved and described in the below article that lowering the case or lemmatizing ruins the precision of these models. \n",
    "\n",
    "\n",
    "<a href='https://medium.com/@dudsdu/named-entity-recognition-for-unstructured-documents-c325d47c7e3a'>Named Entity Recognition for Unstructured Documents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 17,
        "hidden": false,
        "row": 96,
        "width": 5
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When deciding the right term insurance plan, c...</td>\n",
       "      <td>business</td>\n",
       "      <td>Max Life Insurance delivering high claims paid...</td>\n",
       "      <td>Max Life Insurance delivering high claims paid...</td>\n",
       "      <td>max life insurance deliver high claim pay year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra Pradesh has topped the ease of doing bu...</td>\n",
       "      <td>business</td>\n",
       "      <td>Andhra Pradesh tops ease of doing business ran...</td>\n",
       "      <td>Andhra Pradesh tops ease of doing business ran...</td>\n",
       "      <td>andhra pradesh top ease business rank nd year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rajinikanth's wife Latha Rajinikanth will have...</td>\n",
       "      <td>business</td>\n",
       "      <td>Rajinikanth's wife to face trial for fraud ove...</td>\n",
       "      <td>Rajinikanth's wife to face trial for fraud ove...</td>\n",
       "      <td>rajinikanth wife face trial fraud cr unpaid du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online payments company PayPal's UK unit sent ...</td>\n",
       "      <td>business</td>\n",
       "      <td>Being dead is breach of contract, PayPal tells...</td>\n",
       "      <td>Being dead is breach of contract, PayPal tells...</td>\n",
       "      <td>dead breach contract paypal tell deceased pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The world's fifth largest smartphone seller, C...</td>\n",
       "      <td>business</td>\n",
       "      <td>World's 5th largest smartphone seller Xiaomi m...</td>\n",
       "      <td>World's 5th largest smartphone seller Xiaomi m...</td>\n",
       "      <td>world th large smartphone seller xiaomi make i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        news_article news_category  \\\n",
       "0  When deciding the right term insurance plan, c...      business   \n",
       "1  Andhra Pradesh has topped the ease of doing bu...      business   \n",
       "2  Rajinikanth's wife Latha Rajinikanth will have...      business   \n",
       "3  Online payments company PayPal's UK unit sent ...      business   \n",
       "4  The world's fifth largest smartphone seller, C...      business   \n",
       "\n",
       "                                       news_headline  \\\n",
       "0  Max Life Insurance delivering high claims paid...   \n",
       "1  Andhra Pradesh tops ease of doing business ran...   \n",
       "2  Rajinikanth's wife to face trial for fraud ove...   \n",
       "3  Being dead is breach of contract, PayPal tells...   \n",
       "4  World's 5th largest smartphone seller Xiaomi m...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Max Life Insurance delivering high claims paid...   \n",
       "1  Andhra Pradesh tops ease of doing business ran...   \n",
       "2  Rajinikanth's wife to face trial for fraud ove...   \n",
       "3  Being dead is breach of contract, PayPal tells...   \n",
       "4  World's 5th largest smartphone seller Xiaomi m...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  max life insurance deliver high claim pay year...  \n",
       "1  andhra pradesh top ease business rank nd year ...  \n",
       "2  rajinikanth wife face trial fraud cr unpaid du...  \n",
       "3  dead breach contract paypal tell deceased pati...  \n",
       "4  world th large smartphone seller xiaomi make i...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying first 5 rows\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 5,
        "height": 12,
        "hidden": false,
        "row": 96,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>simple_POS_tag</th>\n",
       "      <th>simple_POS_tag_description</th>\n",
       "      <th>detailed_POS_tag</th>\n",
       "      <th>detailed_POS_tag_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nick</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>NNP</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s</td>\n",
       "      <td>PART</td>\n",
       "      <td>particle</td>\n",
       "      <td>POS</td>\n",
       "      <td>possessive ending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ex</td>\n",
       "      <td>X</td>\n",
       "      <td>other</td>\n",
       "      <td>FW</td>\n",
       "      <td>foreign word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>punctuation mark, hyphen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>says</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>MD</td>\n",
       "      <td>verb, modal auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "      <td>adverb</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>compete</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>VB</td>\n",
       "      <td>verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Priyanka</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>NNP</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>:</td>\n",
       "      <td>punctuation mark, colon or ellipsis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Report</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>NNP</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word simple_POS_tag simple_POS_tag_description detailed_POS_tag  \\\n",
       "0         Nick          PROPN                proper noun              NNP   \n",
       "1           's           PART                   particle              POS   \n",
       "2           ex              X                      other               FW   \n",
       "3            -          PUNCT                punctuation             HYPH   \n",
       "4   girlfriend           NOUN                       noun               NN   \n",
       "5         says           VERB                       verb              VBZ   \n",
       "6          she           PRON                    pronoun              PRP   \n",
       "7           ca           VERB                       verb               MD   \n",
       "8          n't            ADV                     adverb               RB   \n",
       "9      compete           VERB                       verb               VB   \n",
       "10        with            ADP                 adposition               IN   \n",
       "11    Priyanka          PROPN                proper noun              NNP   \n",
       "12           :          PUNCT                punctuation                :   \n",
       "13      Report          PROPN                proper noun              NNP   \n",
       "\n",
       "                 detailed_POS_tag_description  \n",
       "0                       noun, proper singular  \n",
       "1                           possessive ending  \n",
       "2                                foreign word  \n",
       "3                    punctuation mark, hyphen  \n",
       "4                      noun, singular or mass  \n",
       "5           verb, 3rd person singular present  \n",
       "6                           pronoun, personal  \n",
       "7                       verb, modal auxiliary  \n",
       "8                                      adverb  \n",
       "9                             verb, base form  \n",
       "10  conjunction, subordinating or preposition  \n",
       "11                      noun, proper singular  \n",
       "12        punctuation mark, colon or ellipsis  \n",
       "13                      noun, proper singular  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly selecting exemplary news_headline for POS tagging\n",
    "doc = str(df_news.loc[np.random.choice(np.arange(len(df_news)), 1).item(), 'news_headline'])\n",
    "doc = nlp(doc)\n",
    "\n",
    "# POS tagging\n",
    "pos_tagged = [(word, word.pos_, spacy.explain(word.pos_), word.tag_, spacy.explain(word.tag_)) for word in doc]\n",
    "df_doc = pd.DataFrame(pos_tagged, columns=['word', 'simple_POS_tag', 'simple_POS_tag_description', 'detailed_POS_tag', 'detailed_POS_tag_description'])\n",
    "df_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 9,
        "height": 15,
        "hidden": false,
        "row": 63,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"2150\" height=\"574.5\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Nick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">ex-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">X</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">girlfriend</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">says</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">she</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">ca</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">compete</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Priyanka:</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">Report</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,177.0 565.0,177.0 565.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M205.0,441.5 L213.0,429.5 197.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,2.0 1975.0,2.0 1975.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M770,439.5 C770,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,441.5 L1453.0,429.5 1437.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1605.0,441.5 L1613.0,429.5 1597.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing dependency visualizer that lets us check model's predictions in Jupyter notebook or browser\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 5,
        "height": 4,
        "hidden": false,
        "row": 108,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a process where an algorithm takes a string of text (sentence or paragraph) as input and identifies relevant nouns that are mentioned in that string which more specifically refer to terms that represent real-world objects like people, places, organizations. Named entity recognition, also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes. Now, let's check how to find them using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Turkish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " chef \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Nusret Gökçe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", who inspired the '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Salt Bae'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " meme after a video of him seasoning meat went viral, has opened a restaurant in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". Among other dishes, a rib-eye at the restaurant costs $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    100\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " while a rack of lamb costs $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    250\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". Meanwhile, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Gökçe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " shows up to slice and season guests' steak during their meal. </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly selecting exemplary news article for NER \n",
    "doc2 = str(df_news.loc[np.random.choice(np.arange(len(df_news)), 1).item(), 'news_article'])\n",
    "doc2 = nlp(doc2)\n",
    "\n",
    "# Finding Named Entities in the article\n",
    "ne_list = [(word, word.ent_type_) for word in doc2 if word.ent_type_]\n",
    "\n",
    "# Visualizing Named Entities\n",
    "displacy.render(doc2, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find out what these entity types exactly mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_type</th>\n",
       "      <th>entity_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>People, including fictional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Countries, cities, states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORP</td>\n",
       "      <td>Nationalities or religious or political groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MONEY</td>\n",
       "      <td>Monetary values, including unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_type                              entity_description\n",
       "0      PERSON                     People, including fictional\n",
       "1         GPE                       Countries, cities, states\n",
       "2        NORP  Nationalities or religious or political groups\n",
       "3         ORG         Companies, agencies, institutions, etc.\n",
       "4       MONEY                 Monetary values, including unit"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting unique types of Named Enities found by Spacy\n",
    "unique_ne_type = set(tup[1] for tup in ne_list)\n",
    "unique_list = [(ne, spacy.explain(ne)) for ne in unique_ne_type]\n",
    "\n",
    "# Dispalying unique types of Named Entities along with their descriptions\n",
    "df_doc2 = pd.DataFrame(unique_list, columns=['entity_type', 'entity_description'])\n",
    "df_doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into the Machine Learning part, let's scan our news corpus and reveal what are the most frequent entities using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a basic pre-processed corpus\n",
    "news_corpus = normalize_corpus(df_news['full_text'], text_lower_case=False, \n",
    "                          text_lemmatization=False, special_char_removal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "for sentence in news_corpus:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        term = word.text\n",
    "        tag = word.ent_type_\n",
    "        if tag:\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                entities_list.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None\n",
    "\n",
    "df_entity = pd.DataFrame(entities_list, columns=['entity_name', 'entity_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>GPE</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>GPE</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress</td>\n",
       "      <td>ORG</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian</td>\n",
       "      <td>NORP</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>two</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>ORG</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UK</td>\n",
       "      <td>GPE</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>one</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>ORG</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>NORP</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Apple</td>\n",
       "      <td>ORG</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORG</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>World Cup</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_name entity_type  Count\n",
       "0           US         GPE   1038\n",
       "1        first     ORDINAL    446\n",
       "2        India         GPE    436\n",
       "3     Congress         ORG    347\n",
       "4       Indian        NORP    299\n",
       "5          two    CARDINAL    295\n",
       "6        Tesla         ORG    189\n",
       "7        China         GPE    186\n",
       "8           UK         GPE    182\n",
       "9          one    CARDINAL    175\n",
       "10    Facebook         ORG    164\n",
       "11     Chinese        NORP    164\n",
       "12       Apple         ORG    161\n",
       "13      Google         ORG    138\n",
       "14   World Cup       EVENT    129"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entity.groupby(by = ['entity_name', 'entity_type']).size().sort_values(ascending = False).reset_index().rename(columns={0: 'Count'})[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 5,
        "height": 4,
        "hidden": false,
        "row": 112,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "TO BE CONTINUED..."
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
